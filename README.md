# Law Base AI Chatbot

This repository contains a **Retrieval-Augmented Generation (RAG) full-stack chatbot application** built with **Python, FastAPI, Redis, React and OpenAI's GPT-4o.** The chatbot specializes in answering questions about nigeria constitution and relevant laws

The application is designed to be easily customizable, allowing you to **integrate your own data sources and adapt it to different use cases.**

You can access a [live demo of the RAG Chatbot application here](https://.com).

## Structure

The repository is organized into two main folders:

- `backend/`: Contains the Python FastAPI backend code and a local Python version for testing.
- `frontend/`: Contains the React frontend code. It uses Vite.js as the build tool and bundler.

## Installation

### Prerequisites âœ…

- Python 3.11+.
- Node.js 18+.
- Poetry (Python package manager).
- Redis Stack Server. Follow the [installation and running instructions](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/). The application requires the RedisJSON and RediSearch modules, which Redis Stack Server includes. Alternatively, you can install Redis and add the required modules yourself.

### Backend

1. Navigate to the backend folder and install the Python dependencies using Poetry:

    ```bash
    cd backend
    poetry install
    ```

2. Create a `.env` file in the backend folder copying the `.env.example` file provided and set the required environment variable:
    - `OPENAI_API_KEY`: Your OpenAI API key.
  
3. The application uses Pydantic Settings for configuration management. You can adjust the configuration defaults in `backend/app/config.py`, or set the configuration variables directly using environment variables.

### Frontend

1. Navigate to the frontend folder and install the JavaScript dependencies:

    ```bash
    cd frontend
    npm install
    ```

2. Create a `.env.development` file in the frontend folder copying the `.env.example` file provided that includes the required environment variable:
    - `VITE_API_URL`: The URL to connect to the backend API.

## Running the Application

### Loading Source Documents

Before running the full-stack application, you need to load the source documents and build the knowledge base. Make sure Redis Stack Server is running before executing the loading script:

```bash
cd backend
poetry run load
```

This script processes the documents in the `backend/data/docs` directory, creates vector embeddings, and stores them in the Redis database.

You can **customize this chatbot with your own data sources:**
1. Replace the existing PDF files in the `backend/data/docs` with your own data sources.
2. If needed, adjust the `process_docs` function in `backend/app/loader.py` to handle different file formats.
3. Adjust the assistant prompts in `backend/app/assistants/prompts.py` for your specific use case.
4. Run the `poetry run load` script as shown above.

### Full-Stack Application

To run the full-stack chatbot application:

1. Ensure Redis Stack Server is running.
   
2. Activate the virtual environment for the backend and start the backend server:

    ```bash
    cd backend
    poetry shell
    fastapi dev app/main.py
    ```

3. In a separate terminal, start the frontend server:

    ```bash
    cd frontend
    npm run dev
    ```

4. Open your web browser and visit `http://localhost:3000` to access the application.

### Local Application

You can run the local Python application for testing in your console using the provided Poetry script:

```bash
cd backend
poetry run local
```

### Exporting Chats

To export all conversation chats to a JSON file in the `backend/data` directory:

```bash
cd backend
poetry run export
```